{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"ctDNA Prospective Trial Report\"\n",
    "execute:\n",
    "  echo: false\n",
    "format:\n",
    "  html:\n",
    "    theme:\n",
    "      light: minty\n",
    "      dark: cyborg\n",
    "    toc: true\n",
    "    embed-resources: true\n",
    "    grid:\n",
    "      sidebar-width: 400px\n",
    "      body-width: 1100px\n",
    "      margin-width: 200px\n",
    "tbl-cap-location: \"top\"\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cfDNA Report template\n",
    "\n",
    "This template contains templat for you to create your own custom report as needed. The parameters cell will get filled in by papermill when\n",
    "the notebook is compiled for each sample. You can fill in those values for one of your samples and build the rest of your analysis as a template.\n",
    "\n",
    "I have left some generic code around, like some examples for calculating cfDNA etc. You will need to go through it all and decide what is of use to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import plotnine as pn\n",
    "import re\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display, Latex\n",
    "import os\n",
    "from datetime import datetime\n",
    "from itables import show\n",
    "from itables import options as opt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.buttons = ['copy', 'excel', 'pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [\n",
    "    \"#279989\",\n",
    "    \"#EAAA00\",\n",
    "    \"#F9423A\",\n",
    "    \"#1D252D\",\n",
    "    \"#D9D9D6\",\n",
    "    \"#1C6E63\",\n",
    "    \"#fb9a99\",\n",
    "    \"#cab2d6\",\n",
    "    \"#33a02c\",\n",
    "    \"#355C7D\",\n",
    "    \"#A8E6CE\",\n",
    "    \"#CC527A\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters, get replaced when notebook is compiled by papermill\n",
    "\n",
    "# Parameters\n",
    "in_notebook = (\n",
    "    \"\"\n",
    ")\n",
    "out_notebook = \"\"\n",
    "maf_files = []\n",
    "samplesheet_path = \"\"\n",
    "patient_id = \"\"\n",
    "hs_metrics = []\n",
    "targ_cov = []\n",
    "repo_path = \"\"\n",
    "completion_files = []\n",
    "\n",
    "lymphgen_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import repo _version\n",
    "sys.path.append(repo_path)\n",
    "from _version import __version__ as pv\n",
    "# import report version\n",
    "from patient_reports._version_report import __version__ as rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex anything after a / and before one of the file extensions\n",
    "def re_sample(file_path):\n",
    "    return re.search(r\"/([^/]+)\\.(?:processed.maf|sage.processed.maf|hs_metrics.txt|target_coverage.txt|completion.txt)\", file_path).group(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn paths into dicts with sample name as the key \n",
    "maf_path_dict = {re_sample(path): path for path in maf_files}\n",
    "hs_path_dict = {re_sample(path): path for path in hs_metrics}\n",
    "sample_points = [sample.split(\"_\")[-1] for sample in maf_path_dict.keys()]\n",
    "cov_path_dict = {re_sample(path): path for path in targ_cov}\n",
    "comp_path_dict = {re_sample(path): path for path in completion_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latest samplesheet\n",
    "samplesheet_all = pd.read_csv(samplesheet_path, sep=\"\\t\")\n",
    "samplesheet = samplesheet_all[samplesheet_all[\"patient_id\"] == patient_id]\n",
    "# remove any normal samples from samplesheet using timepoint\n",
    "# samplesheet = samplesheet[~samplesheet[\"timepoint\"].str.contains(\"normal\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_completion_times(comp_file_dict: dict) -> dict:\n",
    "    comp_times = {}\n",
    "    for sample, comp_file in comp_file_dict.items():\n",
    "        with open(comp_file, \"r\") as f:\n",
    "            comp_times[sample] = f.read().strip()\n",
    "    return comp_times\n",
    "\n",
    "comp_dict = parse_completion_times(comp_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fdf9f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_fdf9f_level0_col0\" class=\"col_heading level0 col0\" >sample_id</th>\n",
       "      <th id=\"T_fdf9f_level0_col1\" class=\"col_heading level0 col1\" >date_collected</th>\n",
       "      <th id=\"T_fdf9f_level0_col2\" class=\"col_heading level0 col2\" >pipeline_time</th>\n",
       "      <th id=\"T_fdf9f_level0_col3\" class=\"col_heading level0 col3\" >TAT (days)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_fdf9f_row0_col0\" class=\"data row0 col0\" >JGH_001_DX</td>\n",
       "      <td id=\"T_fdf9f_row0_col1\" class=\"data row0 col1\" >2024-02-14</td>\n",
       "      <td id=\"T_fdf9f_row0_col2\" class=\"data row0 col2\" >2024-03-22</td>\n",
       "      <td id=\"T_fdf9f_row0_col3\" class=\"data row0 col3\" >37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fdf9f_row1_col0\" class=\"data row1 col0\" >JGH_001_C2</td>\n",
       "      <td id=\"T_fdf9f_row1_col1\" class=\"data row1 col1\" >2024-03-08</td>\n",
       "      <td id=\"T_fdf9f_row1_col2\" class=\"data row1 col2\" >2024-03-22</td>\n",
       "      <td id=\"T_fdf9f_row1_col3\" class=\"data row1 col3\" >14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fdf9f_row2_col0\" class=\"data row2 col0\" >JGH_001_R</td>\n",
       "      <td id=\"T_fdf9f_row2_col1\" class=\"data row2 col1\" >2024-04-30</td>\n",
       "      <td id=\"T_fdf9f_row2_col2\" class=\"data row2 col2\" >2024-05-14</td>\n",
       "      <td id=\"T_fdf9f_row2_col3\" class=\"data row2 col3\" >14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f11b8227150>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| tbl-cap: \"Turn around time\"\n",
    "#| label: tbl-tot\n",
    "\n",
    "tot_df = samplesheet.copy()\n",
    "tot_df = tot_df[~tot_df[\"timepoint\"].str.contains(\"normal\")]\n",
    "# turn date column into datetime\n",
    "tot_df.loc[:, \"date_collected_stamp\"] = pd.to_datetime(tot_df.loc[:,\"date(MM-DD-YYYY)\"], format=\"%m/%d/%Y\")\n",
    "# make version of column that is just the date, looks nicer\n",
    "tot_df.loc[:, \"date_collected\"] = tot_df.loc[:, \"date_collected_stamp\"].dt.date\n",
    "# get date when maf files were created\n",
    "tot_df.loc[:, \"pipeline_time\"] = tot_df[\"sample_id\"].map(comp_dict)\n",
    "# turn date column into datetime\n",
    "tot_df.loc[:, \"pipeline_time_stamp\"] = pd.to_datetime(tot_df.loc[:,\"pipeline_time\"], format=\"%m/%d/%Y\").copy()\n",
    "\n",
    "tot_df.loc[:, \"pipeline_time\"] = tot_df.loc[:, \"pipeline_time_stamp\"].dt.date\n",
    "\n",
    "# create tat column\n",
    "tot_df.loc[:, \"TAT_diff\"] = tot_df[\"pipeline_time_stamp\"]- tot_df[\"date_collected_stamp\"]\n",
    "# convert TOT into a obj column for display purposes\n",
    "tot_df.loc[:, \"TAT (days)\"] = tot_df[\"TAT_diff\"].dt.days\n",
    "\n",
    "# display the table\n",
    "tot_df[['sample_id', 'date_collected', 'pipeline_time',\"TAT (days)\"]].style.hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HS Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hs_metrics(qc_dict: dict):\n",
    "    # use pandas to parse the file but only read in the 7th and 8th line\n",
    "    qc_dfs = []\n",
    "    for sample in qc_dict:\n",
    "        try:\n",
    "            df = pd.read_csv(qc_dict[sample], sep=\"\\t\", skiprows=6, nrows=1)\n",
    "            df['sample_id'] = sample\n",
    "            qc_dfs.append(df)\n",
    "        # exception for when file isnt found, just return none\n",
    "        except FileNotFoundError:\n",
    "            # add warning with purple text\n",
    "            print(f\"HS metrics not found for {sample}\")\n",
    "            return None\n",
    "\n",
    "    return pd.concat(qc_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_cols = ['sample_id','timepoint', 'TOTAL_READS','FOLD_80_BASE_PENALTY','ZERO_CVG_TARGETS_PCT',\n",
    "'MEAN_TARGET_COVERAGE', 'MEDIAN_TARGET_COVERAGE', 'MAX_TARGET_COVERAGE', 'MIN_TARGET_COVERAGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_df = parse_hs_metrics(hs_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hs_df is not None:\n",
    "    hs_df = hs_df.merge(samplesheet[[\"sample_id\",\"timepoint\"]], how=\"left\").copy()\n",
    "    # add in \"normal\" for timepoint where is is missing\n",
    "    hs_df[\"timepoint\"] = hs_df[\"timepoint\"].fillna(\"normal\")\n",
    "    # sort samplesheet by timepoint\n",
    "    hs_df[\"timepoint\"] = pd.Categorical(hs_df[\"timepoint\"], time_order).copy()\n",
    "    hs_df = hs_df.sort_values(\"timepoint\").reset_index(drop=True)\n",
    "    # sort sample_id by timepoint\n",
    "    hs_df[\"sample_id\"] = pd.Categorical(hs_df[\"sample_id\"], hs_df[\"sample_id\"].unique()).copy()\n",
    "\n",
    "    cov_cols = [\"sample_id\" ,'PCT_TARGET_BASES_50X', 'PCT_TARGET_BASES_250X','PCT_TARGET_BASES_500X', 'PCT_TARGET_BASES_1000X']\n",
    "    # change table so coverage percentage is a column\n",
    "    hs_df_cov = hs_df[cov_cols].melt(id_vars=\"sample_id\", value_vars=cov_cols, var_name=\"coverage\", value_name=\"percentage\")\n",
    "\n",
    "    # order coverage column\n",
    "    hs_df_cov[\"coverage\"] = pd.Categorical(hs_df_cov[\"coverage\"], cov_cols).copy()\n",
    "\n",
    "    # turn percentage into actual percentage\n",
    "    hs_df_cov[\"percentage\"] = hs_df_cov[\"percentage\"]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e5f4b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f4b_level0_col0\" class=\"col_heading level0 col0\" >sample_id</th>\n",
       "      <th id=\"T_e5f4b_level0_col1\" class=\"col_heading level0 col1\" >timepoint</th>\n",
       "      <th id=\"T_e5f4b_level0_col2\" class=\"col_heading level0 col2\" >TOTAL_READS</th>\n",
       "      <th id=\"T_e5f4b_level0_col3\" class=\"col_heading level0 col3\" >FOLD_80_BASE_PENALTY</th>\n",
       "      <th id=\"T_e5f4b_level0_col4\" class=\"col_heading level0 col4\" >ZERO_CVG_TARGETS_PCT</th>\n",
       "      <th id=\"T_e5f4b_level0_col5\" class=\"col_heading level0 col5\" >MEAN_TARGET_COVERAGE</th>\n",
       "      <th id=\"T_e5f4b_level0_col6\" class=\"col_heading level0 col6\" >MEDIAN_TARGET_COVERAGE</th>\n",
       "      <th id=\"T_e5f4b_level0_col7\" class=\"col_heading level0 col7\" >MAX_TARGET_COVERAGE</th>\n",
       "      <th id=\"T_e5f4b_level0_col8\" class=\"col_heading level0 col8\" >MIN_TARGET_COVERAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e5f4b_row0_col0\" class=\"data row0 col0\" >JGH_001_DX</td>\n",
       "      <td id=\"T_e5f4b_row0_col1\" class=\"data row0 col1\" >DX</td>\n",
       "      <td id=\"T_e5f4b_row0_col2\" class=\"data row0 col2\" >7189256</td>\n",
       "      <td id=\"T_e5f4b_row0_col3\" class=\"data row0 col3\" >1.317005</td>\n",
       "      <td id=\"T_e5f4b_row0_col4\" class=\"data row0 col4\" >0.003185</td>\n",
       "      <td id=\"T_e5f4b_row0_col5\" class=\"data row0 col5\" >557.093201</td>\n",
       "      <td id=\"T_e5f4b_row0_col6\" class=\"data row0 col6\" >574</td>\n",
       "      <td id=\"T_e5f4b_row0_col7\" class=\"data row0 col7\" >1907</td>\n",
       "      <td id=\"T_e5f4b_row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e5f4b_row1_col0\" class=\"data row1 col0\" >JGH_001_C2</td>\n",
       "      <td id=\"T_e5f4b_row1_col1\" class=\"data row1 col1\" >RCHOP1</td>\n",
       "      <td id=\"T_e5f4b_row1_col2\" class=\"data row1 col2\" >8581812</td>\n",
       "      <td id=\"T_e5f4b_row1_col3\" class=\"data row1 col3\" >1.266880</td>\n",
       "      <td id=\"T_e5f4b_row1_col4\" class=\"data row1 col4\" >0.003583</td>\n",
       "      <td id=\"T_e5f4b_row1_col5\" class=\"data row1 col5\" >623.304815</td>\n",
       "      <td id=\"T_e5f4b_row1_col6\" class=\"data row1 col6\" >654</td>\n",
       "      <td id=\"T_e5f4b_row1_col7\" class=\"data row1 col7\" >1801</td>\n",
       "      <td id=\"T_e5f4b_row1_col8\" class=\"data row1 col8\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e5f4b_row2_col0\" class=\"data row2 col0\" >JGH_001_R</td>\n",
       "      <td id=\"T_e5f4b_row2_col1\" class=\"data row2 col1\" >RELAPSE1</td>\n",
       "      <td id=\"T_e5f4b_row2_col2\" class=\"data row2 col2\" >6058746</td>\n",
       "      <td id=\"T_e5f4b_row2_col3\" class=\"data row2 col3\" >1.424191</td>\n",
       "      <td id=\"T_e5f4b_row2_col4\" class=\"data row2 col4\" >0.003185</td>\n",
       "      <td id=\"T_e5f4b_row2_col5\" class=\"data row2 col5\" >437.226493</td>\n",
       "      <td id=\"T_e5f4b_row2_col6\" class=\"data row2 col6\" >431</td>\n",
       "      <td id=\"T_e5f4b_row2_col7\" class=\"data row2 col7\" >1603</td>\n",
       "      <td id=\"T_e5f4b_row2_col8\" class=\"data row2 col8\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f11b8213c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| tbl-cap: \"HS Metrics\"\n",
    "#| label: tbl-hs\n",
    "\n",
    "if hs_df is not None:\n",
    "    display(hs_df[hs_cols].style.hide())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: \"Percentage of bases achieving different thresholds of coverage, as calculated by picard.\"\n",
    "#| label: fig-hs_cov\n",
    "if hs_df is not None:\n",
    "        plot = (pn.ggplot(hs_df_cov, pn.aes(x=\"coverage\", y=\"percentage\", fill=\"sample_id\")) +\n",
    "                pn.geom_bar(stat=\"identity\", position=\"dodge\") +\n",
    "                pn.theme_bw() +\n",
    "                pn.theme(axis_text_x=pn.element_text(rotation=45, hjust=1)) +\n",
    "                pn.scale_fill_manual(values=COLORS) +\n",
    "                pn.labs(x=\"\", y=\"Percentage of target bases covered\", fill=\"Sample ID\") \n",
    "        )\n",
    "\n",
    "        display(plot.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse coverage files\n",
    "\n",
    "def parse_coverage(path_dict: dict) -> pd.DataFrame: \n",
    "    cov_dfs = []\n",
    "    for sample in path_dict:\n",
    "        df = pd.read_csv(path_dict[sample], sep=\"\\t\")\n",
    "        df[\"sample_id\"] = sample\n",
    "        cov_dfs.append(df)\n",
    "    return pd.concat(cov_dfs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cov = parse_coverage(cov_path_dict)\n",
    "# merge with samplesheet to get timepoint\n",
    "all_cov = all_cov.merge(samplesheet[[\"sample_id\"]], how=\"left\").copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: \"Mean coverage of each capture target per sample.\"\n",
    "#| label: fig-cov\n",
    "\n",
    "# create a coverage boxplot by sample\n",
    "\n",
    "plot = (pn.ggplot(all_cov, pn.aes(x=\"sample_id\", y=\"mean_coverage\", fill=\"sample_id\")) +\n",
    "        pn.geom_violin() +\n",
    "        pn.geom_boxplot(width=0.1) +\n",
    "        pn.theme_bw() +\n",
    "        pn.theme(axis_text_x=pn.element_text(rotation=45, hjust=1)) +\n",
    "        pn.scale_fill_manual(values=COLORS) +\n",
    "        pn.labs(x=\"Sample ID\", y=\"Mean target coverage\", fill=\"Sample ID\") +\n",
    "        # hide legend\n",
    "        pn.theme(legend_position=\"none\")\n",
    "       )\n",
    "\n",
    "plot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets with no coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_cols = ['sample_id','chrom', 'start', 'end', 'length', 'name', '%gc', 'mean_coverage','max_coverage','min_coverage','pct_0x',\n",
    "       'read_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coverage table per sample\n",
    "\n",
    "for sample in all_cov[\"sample_id\"].unique():\n",
    "    sample_cov = all_cov[all_cov[\"sample_id\"] == sample].copy()\n",
    "    display(md(f\"## {sample}\"))\n",
    "    show(sample_cov.loc[sample_cov[\"pct_0x\"] >= 0.5][cov_cols].style.hide())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the mutations\n",
    "\n",
    "def parse_maf_files(maf_dict: dict):\n",
    "    \"\"\"Parse maf files\n",
    "    If file has no lines, then add line with sample_id and timepoint\n",
    "    as placeholder.\n",
    "    \"\"\"\n",
    "    maf_dfs = []\n",
    "    for sample in maf_dict:\n",
    "        # read in 'Tumor_Sample_Barcode' as string\n",
    "        df = pd.read_csv(maf_dict[sample], sep=\"\\t\", dtype={\"Tumor_Sample_Barcode\": str})\n",
    "        if df.empty:\n",
    "            df = pd.DataFrame({\"Tumor_Sample_Barcode\": [sample]})\n",
    "        maf_dfs.append(df)\n",
    "\n",
    "    return pd.concat(maf_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maf_calls = parse_maf_files(maf_path_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no maf calls print empty warning\n",
    "if len(all_maf_calls.columns) == 1:\n",
    "    display(md(\"# No variant calls found for patient. Haulting report compilation\"))\n",
    "    sys.exit(0) # silently haults execution, no need to continue with no variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_maf_calls.rename(columns={'Tumor_Sample_Barcode': 'Sample_ID',\n",
    "                               \"Matched_Norm_Sample_Barcode\": \"Normal_ID\",\n",
    "                               }, inplace=True)\n",
    "\n",
    "all_maf_calls['AF'] = all_maf_calls['t_alt_count'] / all_maf_calls['t_depth']\n",
    "# create version wtih no chip\n",
    "all_maf_calls_nochip = all_maf_calls.loc[(all_maf_calls[\"CHIP\"]==False) | (all_maf_calls[\"CHIP\"].isna())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maf_wmeta_calls[\"variant_id\"] = all_maf_wmeta_calls[\"Hugo_Symbol\"] + \":\" + all_maf_wmeta_calls[\"HGVSp\"].astype(str) + \": \" + all_maf_wmeta_calls[\"Chromosome\"].astype(str)+\":\"+ all_maf_wmeta_calls[\"Start_Position\"].astype(str) +\"-\" + all_maf_wmeta_calls[\"End_Position\"].astype(str)\n",
    "all_maf_wmeta_calls_nochip[\"variant_id\"] = all_maf_wmeta_calls_nochip[\"Hugo_Symbol\"] + \":\" + all_maf_wmeta_calls_nochip[\"HGVSp\"].astype(str) + \": \" + all_maf_wmeta_calls_nochip[\"Chromosome\"].astype(str)+\":\"+ all_maf_wmeta_calls_nochip[\"Start_Position\"].astype(str) +\"-\" + all_maf_wmeta_calls_nochip[\"End_Position\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ctDNA Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hgeml(cfdna_conc: float, mean_AF: float)-> float:\n",
    "    \"\"\"Calculate hGE/mL of ctDNA.\n",
    "\n",
    "    Args:\n",
    "        cfdna_conc: ng/uL of cfDNA extracted\n",
    "        mean_AF: mean AF of variants called in ctDNA.\n",
    "\n",
    "    Returns:\n",
    "        log hGE/mL\n",
    "    \"\"\"\n",
    "    hge_cfdna = (10**6 * cfdna_conc) / 3.3\n",
    "    hge_ctdna = (hge_cfdna * mean_AF)\n",
    "    return numpy.log10(hge_ctdna)\n",
    "\n",
    "# old calculation, if you need to start from ng/uL extraction\n",
    "# mean_af[\"log hGE/mL\"] = mean_af.apply(lambda row: calc_hgeml(row[\"cfDNA_concentration_(ng/uL)\"], row[\"AF\"]), axis=1)\n",
    "\n",
    "# subtract the log hgml frome the previous timepoint for fold change\n",
    "def calc_fold_change(df: pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"Calculate fold change in ctDNA concentration between timepoints.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with log hGE/mL of ctDNA\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with fold change column.\n",
    "    \"\"\"\n",
    "    df[\"log hGE/mL\"] = df[\"log hGE/mL\"].astype(float)\n",
    "    df[\"fold_change\"] = df[\"log hGE/mL\"] - df[\"log hGE/mL\"].shift(1)\n",
    "    # round the value to 2 decimal places\n",
    "    df[\"fold_change\"] = df[\"fold_change\"].round(2)\n",
    "    return df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get table of mean AF per sample\n",
    "mean_af = all_maf_wmeta_calls_nochip.groupby([\"sample_id\"], observed=True)[\"AF\"].mean().reset_index()\n",
    "mean_af = mean_af.merge(samplesheet[[\"sample_id\",\"cfDNA_concentration_(ng/uL)\",\"plasma_volume_mL\",\"HGE/mL_plasma\",\"patient_id\",\"date\"]], how=\"left\").copy()\n",
    "\n",
    "# replace a 0 with NA so it isnt plotted\n",
    "mean_af.replace(0, numpy.nan, inplace=True)\n",
    "\n",
    "# rename AF to Mean AF\n",
    "mean_af.rename(columns={\"AF\": \"Mean AF\"}, inplace=True)\n",
    "\n",
    "# turn \"cfDNA_concentration_(ng/uL)\" to float\n",
    "# mean_af[\"HGE/mL_plasma\"] = mean_af[\"HGE/mL_plasma\"].astype(float)\n",
    "mean_af[\"log hGE/mL\"] = numpy.log10(mean_af[\"HGE/mL_plasma\"] * mean_af[\"Mean AF\"])\n",
    "\n",
    "mean_af.sort_values(\"timepoint\", inplace=True)\n",
    "\n",
    "# if log value is na replace it with a 0\n",
    "mean_af[\"log hGE/mL\"].replace(numpy.nan, 0, inplace=True)\n",
    "\n",
    "# calc fold difference between the current and proceeding timepoint\n",
    "#mean_af[\"fold_change\"] = mean_af.groupby(\"patient_id\")[\"log hGE/mL\"].transform(lambda x: round(x - x.iloc[0],2))\n",
    "mean_af = calc_fold_change(mean_af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| tbl-cap: \"ctDNA concentrations of samples. Fold change is calculated from previous time point. Final value is log hGE/mL and is calculated by log((cfDNA (pg/mL) * mean AF)/ 3.3). TL = too low, UD = undetermined.\"\n",
    "#| label: tbl-ctdna\n",
    "# hide index and round values in plasma volume to 2 decimal places\n",
    "\n",
    "\n",
    "mean_af_cols = [\"sample_id\", \"timepoint\",\"date\", \"Mean AF\",\"cfDNA_concentration_(ng/uL)\",\"plasma_volume_mL\",\"HGE/mL_plasma\", \"log hGE/mL\", \"fold_change\"]\n",
    "\n",
    "conc_table = (mean_af[mean_af_cols]\n",
    ".sort_values(\"date\")\n",
    ".rename(columns={\"cfDNA_concentration_(ng/uL)\": \"cfDNA (ng/uL)\", \"plasma_volume_mL\": \"plasma volume (mL)\", \"HGE/mL_plasma\":\"HGE/mL plasma\",\"fold_change\": \"Fold Change\"})\n",
    ")\n",
    "# round all values to 2 decimal places\n",
    "conc_table = conc_table.round(3)\n",
    "# replace all nan with \"UD\"\n",
    "conc_table[\"Mean AF\"] = conc_table[\"Mean AF\"].astype(\"str\").replace(\"nan\", \"UD\")\n",
    "conc_table[\"Fold Change\"] = conc_table[\"Fold Change\"].astype(\"str\").replace(\"nan\", \"--\")\n",
    "\n",
    "# if cfdna column is float, sometimes it is a string \"TL\"\n",
    "if conc_table[\"cfDNA (ng/uL)\"].dtype == \"float64\":\n",
    "    display(conc_table.style.hide().format({\"plasma volume (mL)\": \"{:.2f}\", \"cfDNA (ng/uL)\": \"{:.3f}\",\"HGE/mL plasma\":\"{:.2f}\",\"log hGE/mL\": \"{:.2f}\"}))\n",
    "else:\n",
    "    display(conc_table.style.hide().format({\"plasma volume (mL)\": \"{:.2f}\",\"HGE/mL plasma\":\"{:.2f}\",\"log hGE/mL\": \"{:.2f}\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: \"ctDNA concentrations across treatment. Fold changes are printed graph and is calculated from previous timepoint.\"\n",
    "#| label: fig-ctdna\n",
    "\n",
    "# calculate fold difference and annotate graph with it\n",
    "mean_af[\"fold_diff\"] = mean_af[\"log hGE/mL\"] - mean_af[\"log hGE/mL\"].shift(1)\n",
    "\n",
    "plot = (pn.ggplot(mean_af, pn.aes(x=\"timepoint_date\", y=\"log hGE/mL\", group=\"patient_id\")) +\n",
    "        pn.geom_point() +\n",
    "        pn.geom_line() +\n",
    "        pn.theme_bw() +\n",
    "        pn.theme(axis_text_x=pn.element_text(rotation=90)) +\n",
    "        pn.scale_fill_manual(values=COLORS) +\n",
    "        pn.labs(x=\"Timepoint\", y=\"log hGE/mL\", fill=\"Sample ID\") +\n",
    "        pn.geom_text(pn.aes(label=\"fold_change\"), nudge_y=0.4) +\n",
    "        # add red dotted line at 0\n",
    "        pn.geom_hline(yintercept=0, linetype=\"dotted\", color=\"red\")\n",
    "\n",
    "       )\n",
    "\n",
    "plot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of interest in maf\n",
    "maf_cols = ['Sample_ID' ,'Hugo_Symbol', 'Chromosome','Start_Position', 'End_Position','Reference_Allele','Tumor_Seq_Allele2', 'AF', 'HGVSp', 'Variant_Classification','dbSNP_RS','Match_Norm_Seq_Allele1',\n",
    "'t_depth','t_ref_count','t_alt_count','n_depth','n_ref_count','n_alt_count',\"LPS\", \"variant_source\",\n",
    "\"origin_samples\", \"UMI_mean\",\"UMI_max\", \"STR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_var_obs(indf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a new line for a timepoint if a given\n",
    "    variant was observed there. Therefore vars from\n",
    "    every timepoint should have a line for every\n",
    "    timepoint. If not observed the AF will be set\n",
    "    to 0.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # get all unique combos of variant_id by timepoint and sample id\n",
    "    uvars = indf[[\"variant_id\"]].drop_duplicates()\n",
    "    utime = indf[[\"sample_id\",\"timepoint\",\"date\",\"timepoint_date\"]].drop_duplicates()\n",
    "    # create a dataframe with all possible combinations of variant_id and timepoint\n",
    "    exp_combos = pd.merge(utime, uvars, how=\"cross\")\n",
    "\n",
    "    # merge the two dataframes\n",
    "    outdf = indf.merge(exp_combos, how=\"right\", on=[\"sample_id\", \"variant_id\", \"timepoint\",\"date\",\"timepoint_date\"]).copy()\n",
    "    # if vaf is na then set it to 0\n",
    "    outdf[\"AF\"] = outdf[\"AF\"].fillna(0)\n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_maf = complete_var_obs(all_maf_wmeta_calls_nochip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_maf.loc[complete_maf[\"variant_id\"]== \"PRKCB:c.422A>T\"][[\"sample_id\", \"timepoint\", \"AF\", \"variant_id\",'Center']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick VAF scale for graph\n",
    "if complete_maf[\"AF\"].max() < 0.05:\n",
    "    vaf_scale = numpy.arange(-0.1, 0.06, 0.005)\n",
    "else:\n",
    "    vaf_scale = numpy.arange(-0.1, 1.1, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: \"Variant allele frequency over time points. The red dotted line indicates a VAF of zero. Variants were obtained from the SAGE output maf files.\"\n",
    "#| label: fig-af\n",
    "\n",
    "# if there are more than 100 variants skip plotting\n",
    "if complete_maf[\"variant_id\"].nunique() > 100:\n",
    "        display(md(\"More than 100 variants found, skipping plot\"))\n",
    "else:\n",
    "        plot = (pn.ggplot(complete_maf, pn.aes(x=\"timepoint_date\", y = \"AF\", group = \"variant_id\" )) +\n",
    "                pn.geom_point() +\n",
    "                pn.geom_line() +\n",
    "                pn.theme_bw() +\n",
    "                pn.scale_fill_manual(values=COLORS) +\n",
    "                pn.labs(x=\"Timepoint\", y=\"Variant allele frequency\", fill=\"Timepoint\") +\n",
    "                pn.scale_y_continuous(breaks=vaf_scale) +\n",
    "                pn.geom_hline(yintercept=0, linetype=\"dashed\", color=\"red\")\n",
    "        )\n",
    "\n",
    "        display(plot.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find midpoint of AF column\n",
    "midvalue = complete_maf[\"AF\"].max() / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by variant\n",
    "complete_maf = complete_maf.sort_values(\"variant_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values so ordered alphabetically on graph\n",
    "# sort variant_id alphabetically, and exclude na values\n",
    "heat_vars = complete_maf.loc[~complete_maf[\"variant_id\"].isna()]\n",
    "\n",
    "heat_vars = heat_vars.sort_values(\"variant_id\", ascending=False).copy()\n",
    "# order variant id\n",
    "heat_vars[\"variant_id\"] = pd.Categorical(heat_vars[\"variant_id\"], heat_vars[\"variant_id\"].unique()).copy()\n",
    "# make sure AF column is a float \n",
    "# heat_vars[\"AF\"] = heat_vars[\"AF\"].astype(float)\n",
    "# turn 0 values to NA\n",
    "heat_vars[\"AF\"] = heat_vars[\"AF\"].replace(0, numpy.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique variant IDs\n",
    "n_vars = len(heat_vars[\"variant_id\"].unique())\n",
    "heat_height = max(round(n_vars/5), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# plot variants across timepoints as heatmap\n",
    "\n",
    "plot = (pn.ggplot(heat_vars, pn.aes(x=\"timepoint_date\", y=\"variant_id\", fill=\"AF\")) +\n",
    "        pn.geom_tile() +\n",
    "        pn.coord_equal(ratio=0.7) +\n",
    "        pn.theme_bw() +\n",
    "        pn.scale_fill_gradient(low=\"white\", high=\"black\") +\n",
    "        pn.theme(axis_text_x=pn.element_text(rotation=90, size=8)) +\n",
    "        pn.labs(x=\"Timepoint\", y=\"Variant ID\", fill=\"Variant allele\\nfrequency\") +\n",
    "        pn.theme(figure_size = (8,heat_height)) +\n",
    "\n",
    "        pn.scale_fill_gradient2(low=\"#91bfdb\", mid=\"#ffffbf\", high=\"#fc8d59\", midpoint=midvalue, na_value=\"white\")\n",
    "       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: \"Heatmap showing variant allele frequencies for all variants across treatment time points. If a grid tile is empty than that variant was not detected at that respective time point. Variants were obtained from the maf output files.\"\n",
    "#| label: fig-af_heatmap\n",
    "\n",
    "# only draw plot if there are less than 100 variants\n",
    "if heat_vars[\"variant_id\"].nunique() < 100:\n",
    "    display(plot.draw())\n",
    "else:\n",
    "    display(md(\"More than 100 variants found, skipping plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| tbl-cap: \"Variants Tables\"\n",
    "#| label: tbl-vars\n",
    "\n",
    "all_maf_wmeta_calls_nochip = all_maf_wmeta_calls_nochip.sort_values(\"timepoint\").reset_index(drop=True).copy()\n",
    "\n",
    "for sample in all_maf_wmeta_calls_nochip[\"Sample_ID\"].unique():\n",
    "    sample_maf = all_maf_wmeta_calls_nochip[all_maf_wmeta_calls_nochip[\"Sample_ID\"] == sample]\n",
    "    display(md(f\"## Sample: {sample}\"))\n",
    "    # display table and replace all _ in columns names with \\n\n",
    "    show(sample_maf[maf_cols], buttons= ['copy', 'excel', 'pdf'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential CHIP mutations\n",
    "\n",
    "Here are any mutations that had a suspicious amount of support in the normal, so was marked as a CHIP mutation and excluded from ctDNA calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id=\"itables_ac5e414f_aaa6_4d3f_ad7a_15050b20e0b7\" class=\"display nowrap\" data-quarto-disable-processing=\"true\" style=\"table-layout:auto;width:auto;margin:auto;caption-side:bottom\">\n",
       "<thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Chromosome</th>\n",
       "      <th>Start_Position</th>\n",
       "      <th>End_Position</th>\n",
       "      <th>Reference_Allele</th>\n",
       "      <th>Tumor_Seq_Allele2</th>\n",
       "      <th>AF</th>\n",
       "      <th>HGVSc</th>\n",
       "      <th>HGVSp</th>\n",
       "      <th>Variant_Classification</th>\n",
       "      <th>dbSNP_RS</th>\n",
       "      <th>Match_Norm_Seq_Allele1</th>\n",
       "      <th>t_depth</th>\n",
       "      <th>t_ref_count</th>\n",
       "      <th>t_alt_count</th>\n",
       "      <th>n_depth</th>\n",
       "      <th>n_ref_count</th>\n",
       "      <th>n_alt_count</th>\n",
       "      <th>LPS</th>\n",
       "      <th>variant_source</th>\n",
       "      <th>origin_samples</th>\n",
       "      <th>CHIP</th>\n",
       "    </tr>\n",
       "  </thead><tbody><tr><td>Loading... (need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td></tr></tbody>\n",
       "\n",
       "</table>\n",
       "<link href=\"https://www.unpkg.com/dt_for_itables@2.0.1/dt_bundle.css\" rel=\"stylesheet\">\n",
       "<script type=\"module\">\n",
       "    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.1/dt_bundle.js';\n",
       "\n",
       "    document.querySelectorAll(\"#itables_ac5e414f_aaa6_4d3f_ad7a_15050b20e0b7:not(.dataTable)\").forEach(table => {\n",
       "        // Define the table data\n",
       "        const data = [];\n",
       "\n",
       "        // Define the dt_args\n",
       "        let dt_args = {\"buttons\": [\"copy\", \"excel\", \"pdf\"], \"layout\": {\"topStart\": \"buttons\", \"topEnd\": null, \"bottomStart\": null, \"bottomEnd\": null}, \"order\": [], \"warn_on_dom\": true};\n",
       "        dt_args[\"data\"] = data;\n",
       "\n",
       "        \n",
       "        new DataTable(table, dt_args);\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(all_maf_wmeta_calls\n",
    ".loc[all_maf_wmeta_calls[\"CHIP\"] == True][maf_cols], buttons= ['copy', 'excel', 'pdf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lymphgen\n",
    "\n",
    "Lymphgen does not work as well without structural variant information, so we expect most samples not to get assigned a subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lg_status(lg_file: str) -> pd.DataFrame:\n",
    "    all_df = []\n",
    "    empty = []\n",
    "    for file in lg_file:\n",
    "        df = pd.read_csv(file, sep=\"\\t\")\n",
    "        all_df.append(df)\n",
    "    \n",
    "    return pd.concat(all_df)\n",
    "\n",
    "lg_df = parse_lg_status(lymphgen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_df.rename(columns={\"Sample.Name\": \"sample_id\"}, inplace=True)\n",
    "# print lg status table\n",
    "\n",
    "lg_df_cols = ['sample_id', 'timepoint',\"Subtype.Prediction\"]\n",
    "lg_df = lg_df.merge(samplesheet[[\"sample_id\",\"timepoint\",\"date\", \"timepoint_date\"]], how=\"outer\").copy()\n",
    "lg_df = lg_df.sort_values(\"timepoint\").reset_index(drop=True).copy()\n",
    "# fill na with None\n",
    "lg_df[\"Subtype.Prediction\"] = lg_df[\"Subtype.Prediction\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| tbl-cap: \"LymphGen Status\"\n",
    "#| label: tbl-lg\n",
    "\n",
    "display(lg_df[lg_df_cols].style.hide())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "___"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mPipeline version: 2.1.0\n",
      "Report version: 1.1.1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "___"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print version info\n",
    "# add long line above and below version info\n",
    "display(md(\"___\"))\n",
    "print('\\033[95m' + f\"Pipeline version: {pv}\" )\n",
    "print(f\"Report version: {rv}\")\n",
    "display(md(\"___\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
